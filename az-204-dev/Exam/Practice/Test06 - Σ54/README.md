# Test06 - Σ54

---

## Questions
|n|Question|Answer|
|-|--------|------|
|1|You develop and deploy a RESTful API to Azure App Service.<br/>You have accessed the API using a browser.<br/>You have received the  below error message.<br/><br/>Failed to load http://api.auzrewebsites.net:60000/#/api/Products: No Access-Control-Allow-Origin header is present on the requested resource.<br/>Origin http://localhost:6000 is therefore not allowed access<br/><br/>What should you do to resolve the error ?<br/><br/>a. Bind an SSL certificate<br/>b. Configure Authentication<br/>c. Enable CORS<br/>d. Add a custom domain<br/>f. Add a CDN|<details><summary>Answer</summary>c. Enable CORS<br/><br/>Because of the domain mismatch between the browser app (http://localhost:6000) and remote resource (http://<app_name>.azurewebsites.net), and the fact that your API in App Service is not sending the Access-Control-Allow-Origin header, your browser has prevented cross-domain content from loading in your browser app. You must enable CORS.<br/>https://docs.microsoft.com/en-us/azure/app-service/app-service-web-tutorial-rest-api#add-cors-functionality</details>|
|2|You plan to create a web application using Azure App Service. The web application must scale horizontally based on the workload.<br/><br/>Which App Service Plan should you consider? The solution must minimize costs.<br/><br/>a. Shared App Service Plan<br/>b. Basic App Service Plan<br/>c. Standard App Service Plan<br/>d. Premium App Service Plan|<details><summary>Answer</summary>c. Standard App Service Plan<br/><br/>The Standard service plan is designed for running production workloads. Pricing is based on the size and number of instances you run. Built-in network load balancing support automatically distributes traffic across instances. The Standard plan includes auto scale that can automatically adjust the number of virtual machine instances running to match your traffic needs.<br/>https://azure.microsoft.com/en-us/pricing/details/app-service/windows/?ref=microsoft.com&utm_source=microsoft.com&utm_medium=docs&utm_campaign=visualstudio</details>|
|3|You have a web application using Azure App Service. You plan to create a CNAME DNS record for the web application.<br/>You need to ensure that users can access the web application by using the https://www.customapplication.com URL.<br/>Which two actions should you perform ?<br/><br/>a. Add a deployment slot to Azure App Service<br/>b. Add a hostname to Azure App Service<br/>c. Scale out the App Service plan<br/>d. Upload a PFX file to Azure App Service|<details><summary>Answer</summary>b. Add a hostname to Azure App Service<br/>d. Upload a PFX file to Azure App Service<br/><br/>You need to register a domain and add it as hostname for the web app. To access the application on https protocol, you need to upload the certificate and add a binding.<br/>https://docs.microsoft.com/en-us/azure/dns/dns-web-sites-custom-domain</details>|
|4|You have deployed an application in an Azure App Service.<br/>You need to analyze the error logs of the application.<br/>Which two methods should you use to view error logs ?<br/><br/>a. Check DetailedErrors inside LogFiules Zip<br/>b. Check Application insider LogFiles Zip<br/>c. Navigate to App Service, Diagnose & solve problems, then search for Failed Request Tracing Logs<br/>d. Navigate to App Service, then check for Activity Logs|<details><summary>Answer</summary>a. CheckDetailedErrors inside LogFiles Zip<br/>c. Navigate to App Service,Diagnose & solve problems,then search for Failed Request Tracing Logs<br/><br/>[<img src="https://i.imgur.com/RzmjZB4.png">](https://i.imgur.com/RzmjZB4.png)<br/>https://docs.microsoft.com/en-us/azure/app-service/troubleshoot-diagnostic-logs</details>|
|5|You are developing a globally distributed application using Azure Cosmos DB. You must ensure that latency must be low for data write operations and require order guarantee between distributed databases.<br/>Which consistency level should you recommend for the database ?<br/><br/>a. Strong<br/>b. Bounded Staleness<br/>c. Session<br/>d. Consistent Prefix<br/>f. Eventual|<details><summary>Answer</summary>b. Bounded Staleness<br/><br/>Bounded staleness is frequently chosen by globally distributed applications that expect low write latencies but require total global order guarantee. The reads are guaranteed to honor the consistent-prefix guarantee. The reads might lag behind writes by at most "K" versions (that is, "updates") of an item or by "T" time interval, whichever is reached first.<br/>https://docs.microsoft.com/en-us/azure/cosmos-db/consistency-levels</details>|
|6|You have deployed an application using Azure Container Instance.<br/>Sometimes, application is taking more time for initial load.<br/>You have verified the logs and noticed that your container is taking a long time to start, but eventually succeeding<br/>Which of the below factors would have caused to take a long start time ?<br/><br/>a. Image size<br/>b. Image location<br/>c. Cached images|<details><summary>Answer</summary>a. Image size<br/>b. Image location<br/><br/>If your container takes a long time to start, but eventually succeeds, start by looking at the size of your container image. Because Azure Container Instances pulls your container image on demand, the startup time you see is directly related to its size.<br/>Another way to reduce the impact of the image pull on your container's startup time is to host the container image in Azure Container Registry in the same region where you intend to deploy container instances. This shortens the network path that the container image needs to travel, significantly shortening the download time.<br/>Azure Container Instances uses a caching mechanism to help speed container startup time<br/>https://docs.microsoft.com/en-us/azure/container-instances/container-instances-troubleshooting#issues-during-container-group-runtime</details>|
|7|You are developing an application to process orders. The Azure Web App pushes the order information into an Azure Queue Storage.<br/>You have an Azure Functions App code that process the orders as shown below.<br/><br/>`public static class OrderProcessor`<br/>`{`<br/>`[FunctionName("ProcessOders")]`<br/>`public static void ProcessOrders([QueueTrigger("incoming-orders")]CloudQueueMessage myQueueitem, [Table("Orders")]ICollector<Order> tableBinding,`<br/>`TraceWriter log)`<br/>`{`<br/>`log.Info($"Processing Order: {myQueueItem.Id}");`<br/>`log.Info($"Queue Insertion Time: {myQueueItem.InsertionTime}");`<br/>`log.Info($"Queue Expiration Time: {myQueueItem.ExpirationTime}");`<br/>`tableBindings.Add(JsonConvert.DeserializeObject<Order>(myQueueItem.AsString));`<br/>`}`<br/>`[FunctionName("ProcessOrders-Poison")]`<br/>`public static void ProcessFailedOrders([QueueTrigger("incoming-orders-poison")]CloudQueueMessage myQueueItem, TraceWriter log)`<br/>`{`<br/>`logError($"Failed to process order: {myQueueItem.AsString}");`<br/>`...`<br/>`}`<br/>`}`<br/><br/>Select Yes if the below statement is true. Otherwise, select No<br/><br/> 1. The Functions App code will log the time that the order was processed from the queue<br/>2. The function will retry up to five times for a given order, including the first try if the ProcessOrders function fails.<br/>3. When there are multiple orders in the queue, a batch of orders will be retrieved from the queue and the ProcessOrders function will run multiple instances concurrently to process the orders.<br/>4. The output of the ProcessOrders function will move the order to an Orders table in Azure Table Storage<br/><br/>a. Yes,Yes,Yes,Yes<br/>b. No,Yes,Yes,Yes<br/>c. No,No,Yes,Yes<br/>d. yes,No,Yes,Yes<br/>e. Yes,No,No,Yes|<details><summary>Answer</summary>b. No,Yes,Yes,Yes<br/><br/>1.  InsertionTime is the time that the message was added to the queue. Not the order processed time<br/>2. By default, Queue triggers retry requests up to five times. After the fifth retry, both the Azure Queue storage and Azure Service Bus triggers write a message to a poison queue<br/>https://docs.microsoft.com/en-us/azure/azure-functions/functions-bindings-error-pages<br/>3. When there are multiple queue messages waiting, the queue trigger retrieves a batch of messages and invokes function instances concurrently to process them. By default, the batch size is 16.<br/>https://docs.microsoft.com/en-us/azure/azure-functions/functions-bindings-storage-queue-trigger?tabs=csharp<br/>4. The output is saved to Orders table in Azure Table Storage</details>|
|8|You have registered a trigger named trigger1 on your Azure Cosmos DB as shown below.<br/><br/>await client.GetContainer("database", "container").Scripts.CreateTriggerAsync(new TriggerProperties<br/>{<br/>Id = "trigger1",<br/>Body = File.ReadAllText("@..\js\trigger1.js"),<br/>TriggerOperation = TriggerOperation.Create,<br/>TriggerType = TriggerType.Post<br/>});<br/>You need to identify when will be the trigger1 executed ?<br/><br/>a. After modifying a database item<br/>b. Before modifying a database item<br/>c. While creating a container|<details><summary>Answer</summary>a. After modifying a database item<br/><br/>Azure Cosmos DB supports pre-triggers and post-triggers. Pre-triggers are executed before modifying a database item and post-triggers are executed after modifying a database item<br/>The code snippet created a post-trigger since trigger type is Pos<br/>https://docs.microsoft.com/en-us/azure/cosmos-db/how-to-write-stored-procedures-triggers-udfs#stored-procedures</details>|
|9|You have registered a trigger named trigger1 on your Azure Cosmos DB as shown below.<br/><br/>await client.GetContainer("database", "container").Scripts.CreateTriggerAsync(new TriggerProperties<br/>{<br/>Id = "trigger1",<br/>Body = File.ReadAllText("@..\js\trigger1.js"),<br/>TriggerOperation = TriggerOperation.Create,<br/>TriggerType = TriggerType.Pre<br/>});<br/><br/>You need to identify when will be the trigger1 executed ?<br/><br/>a. After modifying a database item<br/>b. Before modifying a database item<br/>c. While creating a container|<details><summary>Answer</summary>b. Before modifying a database item<br/><br/>The code snippet created a pre-trigger since trigger type is Pre.<br/>https://docs.microsoft.com/en-us/azure/cosmos-db/how-to-write-stored-procedures-triggers-udfs#stored-procedures</details>|
|10|You are developing an application to create and publish surveys.<br/><br/>The application will use Azure Active Directory (Azure AD) for authentication.<br/>The application will have a front-end web app and a back-end web API.<br/>The web API will handle the requests from the front-end web app. The web app must authenticate with web API by using OAuth2.0 bearer tokens. The web app must authenticate by using the identities of individual users<br/><br/>What should you consider to generate access tokens ?<br/><br/>a. AAD<br/>b. A Web App<br/>c. A Web API|<details><summary>Answer</summary>a. AAD<br/><br/>1. AAD is a comprehensive identity and access management solution provided by Microsoft. It allows you to manage user identities and access to resources in Azure and other Microsoft services.<br/>2. For your scenario, you'll need to configure your front-end web app to integrate with Azure AD for authentication. This integration will allow individual users to log in to your web app using their Azure AD identities.<br/>3. Once the user is authenticated, the front-end web app will request an access token from Azure AD to access the back-end web API. This access token will be a bearer token that authorizes the web app to make requests on behalf of the authenticated user.<br/>4. The back-end web API should be protected and configured to validate the incoming access tokens from the front-end web app to ensure the requests are legitimate</details>|
|11|You are developing an application to create and publish surveys.<br/>The application will use Azure Active Directory (Azure AD) for authentication.<br/>The application will have a front-end web app and a back-end web API.<br/>The web API will handle the requests from the front-end web app. The web app must authenticate with web API by using OAuth2.0 bearer tokens. The web app must authenticate by using the identities of individual users.<br/><br/>What should you consider to authorize decisions based on access token ?<br/><br/>a. AAD<br/>b. A Web App<br/>c. A Web API|<details><summary>Answer</summary>c. A Web API<br/><br/>You can authorize the access based on access token in Web API<br/>1. The front-end web app will obtain an access token from Azure Active Directory (Azure AD) after a user logs in and grants the necessary permissions.<br/>2. This access token will be included in the HTTP requests made by the front-end web app to the back-end web API to access protected resources<br/>3. The back-end web API, being the service that handles requests from the web app, needs to validate and authorize the incoming requests based on the access token.<br/>4. When the back-end web API receives a request with an access token, it should validate the token's authenticity and perform the necessary checks to ensure that the user associated with the token has the appropriate permissions to perform the requested action.<br/>5. Azure AD provides a mechanism for validating access tokens, and your web API should leverage this capability to authorize decisions based on the received tokens<br/>https://docs.microsoft.com/en-us/azure/active-directory/develop/quickstart-v2-dotnet-native-aspnet</details>|
|12|You are developing an application using Azure web app and Azure SQL Server.<br/>You need to implement dynamic data masking on a column that stores social security numbers as a string in the format "123-45-6789”.<br/>You want to mask everything except the last four digits.<br/><br/>Which data masking function and replacement string should you choose to fulfill this requirement ?<br/><br/>a. Dfault, "XXXX"<br/>b. Text,"XXXX-XX-"<br/>c. Credit card, "XXXX-XXXX-"<br/>d. Email,"XXXX-XX-"|<details><summary>Answer</summary>b. Text,"XXXX-XX"<br/><br/>Use Text, adds a custom padding string and number of characters to be exposed prefix and suffix.<br/>1. Dynamic Data Masking in Azure SQL Database allows you to define masking rules on specific columns to protect sensitive data from unauthorized access. It can dynamically mask the data as per the defined rules, and the original data is not altered in the database.<br/>The "Text" data masking function is used to mask a portion of the data while keeping some characters visible. In this case, "XXXX-XX-" will mask the first five digits of the social security number and show only the last four digits as plain text.<br/>3. Here's how the masking will be applied to the social security number "123-45-6789"<br/>Original: 123-45-6789<br/>Masked: XXX-XX-6789<br/>4. This way, you can achieve the requirement of displaying only the last four digits of the social security number while masking the rest<br/>https://docs.microsoft.com/en-us/azure/azure-sql/database/dynamic-data-masking-overview</details>|
|13|You have an Azure SQL Database instance. Database management is performed by an external party. All cryptographic keys are stored in an Azure Key Vault. You must ensure that the external party cannot access the data in the SSN column of the Person Table.<br/>To achieve the requirement, you store column encryption keys in the system catalog view in the database.<br/>Did you achieve the requirement ?<br/><br/>a. Yes<br/>b. No|<details><summary>Answer</summary>b. No<br/><br/>Storing column encryption keys in the system catalog view of the database does not ensure that the external party cannot access the data in the SSN column of the Person Table. The system catalog views in the database are accessible to users with appropriate permissions, and if the external party has the necessary privileges, they could potentially access the keys and decrypt the data.<br/><br/>To achieve the requirement of preventing the external party from accessing the data in the SSN column, you should use Always Encrypted with Azure Key Vault integration. With Always Encrypted, the data in the SSN column is encrypted on the client-side before it is sent to the database. The cryptographic keys required for encryption and decryption are stored securely in Azure Key Vault and are never revealed to the database system or the external party. This way, the database administrators or the external party won't be able to access the sensitive data in the SSN column even if they have access to the database<br/><br/>Always Encrypted provides a strong layer of data protection, and by integrating it with Azure Key Vault, you can ensure that sensitive data remains encrypted and secure from unauthorized access.<br/>https://docs.microsoft.com/en-us/azure/azure-sql/database/security-overview</details>|
|14|You have an Azure SQL Database instance.<br/>Database management is performed by an external party.<br/>All cryptographic keys are stored in an Azure Key Vault.<br/>You must ensure that the external party cannot access the data in the SSN column of the Person Table.<br/>To achieve the requirement, you enable always encrypted feature.<br/>Did you achieve the requirement ?<br/><br/>a. yes<br/>b. No|<details><summary>Answer</summary>a. Yes<br/><br/>Always Encrypted is a feature designed to protect sensitive data stored in specific database columns from access (for example, credit card numbers, national identification numbers, or data on a need to know basis). This includes database administrators or other privileged users who are authorized to access the database to perform management tasks, but have no business need to access the particular data in the encrypted columns<br/>https://docs.microsoft.com/en-us/azure/azure-sql/database/security-overview</details>|
|15|You have an Azure App Services Web App, Azure SQL Database instance in a resource group.<br/>A developer wants to publish code to the web app.<br/>You must grant the developer with Contribute role to the web app.<br/>Select the two commands should you use. Each correct answer presents a complete solution<br/><br/>a. az role assignment create<br/>b. az role definition create<br/>c. New-AzureRmRoleAssignment<br/>d. New-AzureRmRoleDefinition|<details><summary>Answer</summary>a. az role assignment create<br/>c. New-AzureRmRoleAssignment<br/><br/>az role assignment create - Create a new role assignment for a user, group, or service principal.<br/>https://docs.microsoft.com/en-us/cli/azure/role/assignment?view=azure-cli-latest#az-role-assignment-create<br/>New-AzureRmRoleAssignment - Assigns the specified RBAC role to the specified principal, at the specified scope.<br/>htps://docs.microsoft.com/en-us/powershell/module/azurerm.resources/new-azurermroleassignment?view=azurermps-6.13.0</details>|
|16|You have created below caching rules on an Azure CDN endpoint named staticcontent.azureedge.net<br/>• Global caching rule:<br/>o Caching behavior: Override<br/>o Cache expiration duration: 1 day<br/>• Custom caching rule #1:<br/>o Match condition: Path<br/>o Match value: /home/*<br/>o Caching behavior: Override<br/>o Cache expiration duration: 2 days<br/>• Custom caching rule #2:<br/>o Match condition: Extension<br/>o Match value: .html<br/>o Caching behavior: Set if missing<br/>o Cache expiration duration: 3 days<br/><br/>A user requested for staticcontent.azureedge.net/home/index.html.<br/><br/>For how many days the index.html file will be cached ?<br/><br/>a. 1<br/>b. 2<br/>c. 3|<details><summary>Answer</summary>c. 3<br/><br/>When these rules are set, a request for <endpoint hostname>.azureedge.net/home/index.html triggers custom caching rule #2, which is set to: Set if missing and 3 days. Therefore, if the index.html file has Cache-Control or Expires HTTP headers, they are honored; otherwise, if these headers are not set, the file is cached for 3 days.<br/>https://docs.microsoft.com/en-us/azure/cdn/cdn-caching-rules</details>|
|17|You have created below caching rules on an Azure CDN endpoint named staticcontent.azureedge.net<br/><br/>• Global caching rule:<br/>o Caching behavior: Override<br/>o Cache expiration duration: 1 day<br/>• Custom caching rule #1:<br/>o Match condition: Path<br/>o Match value: /home/*<br/>o Caching behavior: Override<br/>o Cache expiration duration: 2 days<br/>• Custom caching rule #2:<br/>o Match condition: Extension<br/>o Match value: .html<br/>o Caching behavior: Set if missing<br/>o Cache expiration duration: 3 days<br/><br/>Which caching rule takes precedence ?<br/><br/>a. Global caching rule<br/>b. Custom caching rule#1<br/>c. Custom caching rule#2|<details><summary>Answer</summary>c. Custom caching rule#2<br/><br/>Global and custom caching rules are processed in the following order:<br/>Global caching rules take precedence over the default CDN caching behavior (HTTP cache-directive header settings).<br/>Custom caching rules take precedence over global caching rules, where they apply. Custom caching rules are processed in order from top to bottom. That is, if a request matches both conditions, rules at the bottom of the list take precedence over rules at the top of the list. Therefore, you should place more specific rules lower in the list.<br/>Https://docs.microsoft.com/en-us/azure/cdn/cdn-caching-rules</details>|
|18|You have an application that stores the diagnostic logs in an Azure Storage account.<br/>What should you use to retrieve the diagnostics logs ?<br/><br/>a. SQL query editor<br/>b. File Explorer<br/>c. AzCopy|<details><summary>Answer</summary>c. AzCopy<br/><br/>AzCopy is used to upload or retrieve data from storage account. In this scenario, logs are getting stored in storage account. So, to retrieve logs, we must use AzCopy<br/>https://docs.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-v10</details>|
|19|You have developed an application using Azure Web App.<br/>You need to verify that your application is available on average every minute.<br/><br/>Which three actions should you perform in sequence ?<br/><br/>a. Create an Application Insights<br/>Create a test<br/>Provide frequency as 5 minutes & 5 test locations<br/><br/>b. Create an Application Insights<br/>Provide frequency as 1 minute & 5 test locations<br/><br/>c. Create an Application Insights<br/>Create a test<br/>Provide frequency as 5 minutes & 1 test location|<details><summary>Answer</summary>a. Create an Applicatioons Insights<br/>Create a test<br/>Provide frequency as 5 minutes & 5 test locations<br/><br/>Create an Application Insights resource<br/>Create a URL ping test<br/>Test frequency - Sets how often the test is run from each test location. With a default frequency of five minutes and five test locations, your site is tested on average every minute.<br/>https://docs.microsoft.com/en-us/azure/azure-monitor/app/monitor-web-app-availability</details>|
|20|You are plan to publish APIs by using Azure API Management. You need to configure the forward-request policy to forward the incoming request to the backend service specified in the request context.<br/><br/>Which policy section should you use ?<br/><br/>a. Oubound<br/>b. Backed<br/>c. on-error<br/>d. inbound|<details><summary>Answer</summary>b. Backend<br/><br/>The forward-request policy forwards the incoming request to the backend service specified in the request context. The backend service URL is specified in the API settings and can be changed using the set backend service policy. The following API level policy forwards all API requests to the backend service with a timeout interval of 60 seconds.<br/>`<!-- api level -->`<br/>`<policies>`<br/>`<inbound>`<br/>`<base/>`<br/>`</inbound>`<br/>`<backend>`<br/>`<forward-request timeout="60"/>`<br/>`</backend>`<br/>`<outbound>`<br/>`<base/>`<br/>`</outbound>`<br/>`</policies>`<br/>https://docs.microsoft.com/en-us/azure/api-management/api-management-advanced-policies#ForwardRequest</details>|
|21|You provide an Azure API Management web service to your clients.<br/>The back-end web service implements HTTP Strict Transport Security (HSTS).<br/>Every request to the backend service must include a valid HTTP authorization header.<br/>You need to configure the Azure API Management instance with an authentication policy.<br/><br/>Which two policies can you use ?<br/><br/>a. OAuth Client Credential Grant<br/>b. Basic Authentication<br/>c. Certificate Authentication<br/>d. Digest Authentication|<details><summary>Answer</summary>b. Basic Authentication<br/>c. Certificate Authentication<br/><br/>Authenticate with Basic - Authenticate with a backend service using Basic authentication. This policy effectively sets the HTTP Authorization header to the value corresponding to the credentials provided in the policy.<br/>Authenticate with client certificate - Authenticate with a backend service using client certificates.<br/>Authenticate with managed identity - Authenticate with the managed identity for the API Management service.<br/>https://docs.microsoft.com/en-us/azure/api-management/api-management-authentication-policies</details>|
|22|You are planning to publish APIs for your clients by using Azure API Management. You need to restrict incoming requests from a specified IP addresses.<br/><br/>Which policy should you configure ?<br/><br/>a. Check HTTP Header<br/>b. Restrict Caller lps<br/>c. Restrict incoming lps<br/>d. Validte JWT|<details><summary>Answer</summary>b. Restrict Caller lps<br/><br/>To add a new statement to restrict incoming requests to specified IP addresses, place the cursor just inside the content of the inbound XML element and click the Restrict caller IPs statement.<br/>e.g:<br/>`<policies>`<br/>`    <inbound>`<br/>`        <base />`<br/>`        <restrict-caller-ips>`<br/>`            <allow>`<br/>`                <ip-address range="xxx.xxx.xxx.xxx" />`<br/>`                <!-- Additional IP addresses or ranges can be added here -->`<br/>`            </allow>`<br/>`            <!-- Optionally, you can use <deny> to block specific IPs -->`<br/>`            <!-- <deny>`<br/>`                <ip-address range="yyy.yyy.yyy.yyy" />`<br/>`                <ip-address range="zzz.zzz.zzz.zzz" />`<br/>`            </deny> -->`<br/>`        </restrict-caller-ips>`<br/>`    </inbound>`<br/>`    <backend>`<br/>`        <base />`<br/>`    </backend>`<br/>`    <outbound>`<br/>`        <base />`<br/>`    </outbound>`<br/>`    <on-error>`<br/>`        <base />`<br/>`    </on-error>`<br/>`</policies>`<br/>https://docs.microsoft.com/en-us/azure/api-management/api-management-howto-policies#restrict-incoming-requests</details>|
|23|You develop an online shopping mobile application.<br/>You need to implement push notifications to send coupons for interested users on various conditions.<br/>Which of the below conditions you can use to send customized coupons for your users ?<br/><br/>a. Coupons to specific devices<br/>b. Coupons to specific location users<br/>c. Coupons to specific users|<details><summary>Answer</summary>a. Coupons to specific devices<br/>b. Coupons to specific location users<br/>c. Coupons to specific users<br/><br/>We can send push notifications to specific users or specific location-based notifications or to specific devices<br/>https://docs.microsoft.com/en-us/azure/notification-hubs/notification-hubs-windows-store-dotnet-get-started-wns-push-notification</details>|
|24|You plan to develop a solution using Azure Event Hubs. You need to create a resource group, Event Hubs namespace, and an Event Hub using Azure CLI.<br/><br/>Which three commands should you use ?<br/><br/>a. az group create<br/>b. az eventhubs namespace create<br/>c. az eventhubs namespace update<br/>d. az eventhubs eventhub update<br/>e. az eventhubs eventhub create|<details><summary>Answer</summary>a. az group create<br/>b. az eventhubs namespace create<br/>e. az eventhubs eventhub create<br/><br/>A resource group is a logical collection of Azure resources.<br/>An Event Hubs namespace provides a unique scoping container, referenced by its fully qualified domain name, in which you create one or more event hubs<br/>https://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-tutorial-visualize-anomalies</details>|
|25|You plan to create a Docker image that runs an ASP.NET Core application named MyApp.<br/>You have a setup script named setupScript.ps1 and a series of application files including MyApp.dll.<br/><br/>You need to create a Dockerfile document that meets the following requirements:<br/>Call setupScripts.ps1 when the container is built.<br/>Run MyApp.dll when the container starts.<br/><br/>The Dockerfile document must be created in the same folder where MyApp.dll and setupScript.ps1 are stored.<br/><br/>Select the below commands in sequence should you use to develop the solution ?<br/><br/>1. WORKDIR /apps/MyApp<br/>2. FROM microsoft/aspnetcore:latest<br/>3. RUN powershell ./setupScript.ps1<br/>4. Copy ./ .<br/>5. CMD [“dotnet”, “MyApp.dll”]<br/><br/>a. 2,1,4,5,3<br/>b. 2,1,4,3,5<br/>c. 1,4,2,3,5|<details><summary>Answer</summary>b. 2,1,4,3,5<br/><br/>The FROM statement in the Dockefile specify the image to use as the base image.<br/>Then specify the working directory<br/>Then copy the application contents using the COPY command<br/>Then use the commands to run the PowerShell command and the ENTRYPOINT statement to run the dotnet application.<br/>https://docs.microsoft.com/en-us/aspnet/core/host-and-deploy/docker/building-net-docker-images?view=aspnetcore-3.1</details>|
|26|You have created a development environment in Azure using the latest Visual Studio image from the Azure Marketplace.<br/><br/>You have customized the virtual machine (VM) for your development environment by installing the third party software’s and software development kits.<br/>You need to save the customized VM to allow provisioning of a new team member development environment.<br/><br/>Which tools or services should you use to generalize the VM ?<br/><br/>a. Azure PowerShell<br/>b. Visual Studio command prompt<br/>c. Azure Migrate<br/>d. Azure Backup|<details><summary>Answer</summary>a. Azure PowerShell<br/><br/>e.g:<br/>`Connect-AzAccount`<br/>`Select-AzSubscription -SubscriptionName "Nom de l'abonnement"`<br/>`Set-AzVM -ResourceGroupName "NomResourceGroup" -Name "NomVM" -Generalized`<br/><br/>Generalized(sysPrep)<br/>https://docs.microsoft.com/en-us/azure/virtual-machines/windows/capture-image-resource#create-an-image-of-a-vm-using-powershell</details>|
|27|You have created a development environment in Azure using the latest Visual Studio image from the Azure Marketplace.<br/>You have customized the virtual machine (VM) for your development environment by installing the third party software’s and software development kits.<br/>You need to save the customized VM to allow provisioning of a new team member development environment.<br/>Which service should you use to store images ?<br/><br/>a. Azure Blob Storage<br/>b. Azure Data Lake Storage<br/>c. Azure File Storage<br/>d. Azure Table Storage|<details><summary>Answer</summary>a. Azure Blob Storage<br/><br/>You can store images in Azure Blob Storage or can also share with Image gallery.<br/>Azure Blob Storage is designed for storing unstructured data, including VM images, documents, backups, and media files. VM images can be saved as VHD (Virtual Hard Disk) files, and these VHD files can be stored in Azure Blob Storage. Once the VHD file is stored, you can use it as a custom image to create new VMs.<br/>https://docs.microsoft.com/en-us/azure/virtual-machines/shared-image-galleries</details>|
|28|You have an application that upload photos to an Azure Storage account of type General purpose V2.<br/><br/>You plan to produce a mobile-friendly version of the photo once it is uploaded to storage account. The process must start in less than one minute.<br/><br/>To achieve the above requirement, you create an Azure Function app that uses the Consumption hosting model and that is triggered from the blob upload.<br/><br/>Did you achieve the requirement ?<br/><br/>a. Yes<br/>b. No|<details><summary>Answer</summary>a. Yes<br/><br/>Azure Storage events allow applications to react to events, such as the creation and deletion of blobs. It does so without the need for complicated code or expensive and inefficient polling services. The best part is you only pay for what you use.<br/>Blob storage events are pushed using Azure Event Grid to subscribers such as Azure Functions, Azure Logic Apps, or even to your own http listener. Event Grid provides reliable event delivery to your applications through rich retry policies and dead-lettering.<br/>https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-event-overview</details>|
|29|Your team plan to push images into an Azure Container Registry using CI/CD pipeline. The images will be pulled to other Azure services.<br/><br/>Which authentication method should you use ?<br/><br/>a. Individual AD identity<br/>b. AD service principal<br/>c. Repository-scoped access token<br/>d. Managed identity for Azure resources<br/>e. Admin user|<details><summary>Answer</summary>b. AD service principal<br/>d. Managed identity for Azure resources<br/><br/>[<img src="https://i.imgur.com/80614I5.png">](https://i.imgur.com/80614I5.png)<br/>https://docs.microsoft.com/en-us/azure/container-registry/container-registry-authentication</details>|
|30|You need to update the metadata of the blobs in an Azure blob storage.<br/>Which three methods should you use ?<br/><br/>a. GetPropertiesAsync<br/>Metadata.Add<br/>UploadFileStream<br/><br/>b. GetPropertiesAsync<br/>Metadata.Add<br/>SetMetadataAsync<br/><br/>c. GetPropertiesAsync<br/>Metadata.Update<br/>SetMetadataAsync|<details><summary>Answer</summary>b. GetPropertiesAsync<br/>Metadata.Add<br/>SetMetadataAsync<br/><br/>GetPropertiesAsync - Get the blob's properties and metadata<br/>Metadata.Add - Add metadata to the dictionary by calling the Add method<br/>SetMetadataAsync - Set the blob's metadata<br/>https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-properties-metadata?tabs=dotnet#set-and-retrieve-metadata</details>|
|31|Your company has 2000 stores located throughout the world. Additional stores are expected to open in the future.<br/><br/>You are developing an application to collect point-of-sale (POS) device data. Each store location may have one to five devices that send data.<br/><br/>Each device can produce 2 megabytes (MB) of data every 24 hours.<br/>Azure blob storage is used to store the device data in Azure.<br/>Device data must be correlated based on a device identifier.<br/>You need to implement a solution to receive the device data.<br/><br/>To achieve the requirement, you provision an Azure Notification Hub and register all devices with the hub.<br/>Does the solution meet the goal ?<br/><br/>a. Yes<br/>b. No|<details><summary>Answer</summary>b. No<br/><br/>Azure Notification Hub is used to Send push notifications from any backend to any mobile device.<br/>https://docs.microsoft.com/en-us/azure/notification-hubs/</details>|
|32|Your company has 2000 stores located throughout the world. Additional stores are expected to open in the future<br/><br/>You are developing an application to collect point-of-sale (POS) device data. Each store location may have one to five devices that send data.<br/><br/>Each device can produce 2 megabytes (MB) of data every 24 hours.<br/>Azure blob storage is used to store the device data in Azure.<br/>Device data must be correlated based on a device identifier.<br/>You need to implement a solution to receive the device data.<br/><br/>To achieve the requirement, you provision an Azure Service Bus and configure a topic to receive the device data by using a correlation filter.<br/><br/>Does the solution meet the goal ?<br/><br/>a. Yes<br/>b. No|<details><summary>Answer</summary>b. No<br/><br/>Azure Event Grid is for managing routing of all events from any source to any destination<br/>https://docs.microsoft.com/en-us/azure/event-grid/overview</details>|
|33|Your company has 2000 stores located throughout the world. Additional stores are expected to open in the future.<br/><br/>You are developing an application to collect point-of-sale (POS) device data. Each store location may have one to five devices that send data.<br/><br/>Each device can produce 2 megabytes (MB) of data every 24 hours.<br/>Azure blob storage is used to store the device data in Azure.<br/>Device data must be correlated based on a device identifier.<br/>You need to implement a solution to receive the device data.<br/><br/>To achieve the requirement, you provision an Azure Event Grid. Configure the machine identifier as the partition key and enable capture.<br/><br/>Does the solution meet the goal ?<br/><br/>a. Yes<br/>b. No|<details><summary>Answer</summary>b. No<br/><br/>Azure Event Grid is for managing routing of all events from any source to any destination<br/>https://docs.microsoft.com/en-us/azure/event-grid/overview</details>|
|34|You are developing a .NET application that receives a message each time an Azure virtual machine finishes processing data<br/><br/>The receiving application must NOT persist the message after processing.<br/><br/>Which object should you use to implement the .NET object that will receive the messages ?<br/><br/>a. QueueClient<br/>b. SubscriptionClient<br/>c. TopicClient<br/>d. CloudQueueClient|<details><summary>Answer</summary>a. QueueClient<br/>The QueueClient class enables you to retrieve queues stored in Queue Storage.<br/>https://docs.microsoft.com/en-us/azure/storage/queues/storage-dotnet-how-to-use-queues?tabs=dotnet#create-the-queue-storage-client</details>|
|35|You have an Azure Storage account. You need to copy large volumes of data from the existing storage account to a new storage account.<br/><br/>The copy process must meet the following requirements:<br/>Automate data movement.<br/>Minimize user input required to perform the operation.<br/>Ensure that the data movement process is recoverable.<br/><br/>What should you use ?<br/><br/>a. AzCopy<br/>b. Azure Storage Explorer<br/>c. Azure portal<br/>d. .net Storage Client Library|<details><summary>Answer</summary>a. azCopy<br/><br/>AzCopy is a command-line utility that you can use to copy files to or from a storage account.<br/>https://docs.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-files#synchronize-filesv</details>|
|36|You are developing a web application using ASP.NET Core.<br/>The web application is used to provide weather data sets that are refreshed every 8 hours.<br/>Data sets are basically a Comma Separated Value (CSV) files.<br/>The web application uses Azure FrontDoor.<br/>Some files must be purged from the FrontDoor cache based upon Response Header values.<br/><br/>Which type of cache purge should you use to purge individual assets from the Front Door cache ?<br/><br/>a. single path<br/>b. wildcard<br/>c. root domain|<details><summary>Answer</summary>a. single path<br/><br/>Front Door caches assets until the asset's time-to-live (TTL) expires. Whenever a client requests an asset with expired TTL, the Front Door environment retrieves a new updated copy of the asset to serve the request and then stores the refreshed cache. Sometimes you may wish to purge cached content from all edge nodes and force them all to retrieve new updated assets. Single path purge: Purge individual assets by specifying the full path of the asset (without the protocol and domain), with the file extension, for example, /pictures/strasbourg.png;<br/>The scenario in the question asks to purge individual and specific files. So, single path purge is the right choice<br/>https://docs.microsoft.com/en-us/azure/frontdoor/front-door-caching#cache-purge</details>|
|37|You are developing an application for a company that has multiple warehouses.<br/>Each warehouse contains IoT temperature devices.<br/>Temperature data is delivered to an Azure Service Bus queue.<br/><br/>You need to send email alerts to warehouse supervisors immediately if the temperature at a warehouse goes above or below specified threshold temperatures.<br/><br/>Select the below five actions should you perform in sequence ?<br/><br/>1. Create a blank Logic app<br/>2. Add a logical app action that fires when one or more messages arrive in the queue<br/>3. Add an action that reads IoT temperature data from the Service Bus queue<br/>4. Add a condition that compares the temperature against the upper and lower thresholds<br/>5. Add an action that sends an email to specified personnel if the temperature is outside of those thresholds<br/>6. Add a logical app trigger that fires when one or more messages arrive in the queue<br/>7. Add a recurrence trigger that schedules the app to run every 15 minutes<br/><br/>a. 1,6,3,4,5<br/>b. 1,2,3,4,5<br/>c. 1,7,3,2,5<br/>d. 2,3,6,7,5|<details><summary>Answer</summary>a. 1,6,3,5<br/><br/>The steps are as follows<br/>Create a blank Logic app<br/>Add a logical app trigger that fires when one or more messages arrive in the queue<br/>Add an action that reads IoT temperature data from the Service Bus queue<br/>Add a condition that compares the temperature against the upper and lower thresholds<br/>Add an action that sends an email to specified personnel if the temperature is outside of those thresholds<br/>https://docs.microsoft.com/en-us/azure/iot-hub/iot-hub-monitoring-notifications-with-azure-logic-apps#create-and-configure-a-logic-app</details>|
|38|You develop an ASP.NET Core MVC application. You need to identify trends in application usage.<br/>You configure the application to track webpages and custom events.<br/>You need to find which pages visited by users most often correlate to a product purchase.<br/>Which Azure Application Insights Usage Analysis features should you use ?<br/><br/>a. Users<br/>b. Funnels<br/>c. Impact<br/>d. Retention<br/>e. User Flows|<details><summary>Answer</summary>b. Funnels<br/><br/>If your application involves multiple stages, you need to know if most customers are progressing through the entire process, or if they are ending the process at some point. The progression through a series of steps in a web application is known as a funnel. You can use Azure Application Insights Funnels to gain insights into your users, and monitor step-by-step conversion rates.<br/>https://docs.microsoft.com/en-us/azure/azure-monitor/app/usage-funnels</details>|
|39|You develop an ASP.NET Core MVC application. You need to identify trends in application usage.<br/>You configure the application to track webpages and custom events.<br/>You need to find the impact of product display page load time on the user’s decision to purchase a product.<br/>Which Azure Application Insights Usage Analysis features should you use ?<br/><br/>a. Users<br/>b. Funnels<br/>c. Impact<br/>d. Retention<br/>e. User Flows|<details><summary>Answer</summary>c. Impact<br/><br/>Impact analyzes how load times and other properties influence conversion rates for various parts of your app.<br/>https://docs.microsoft.com/en-us/azure/azure-monitor/app/usage-impact</details>|
|40|You develop an ASP.NET Core MVC application. You need to identify trends in application usage.<br/>You configure the application to track webpages and custom events.<br/>You need to find the events that most influence a user’s decision to continue to use the application.<br/>Which Azure Application Insights Usage Analysis features should you use ?<br/><br/>a. Users<br/>b. Funnels<br/>c. Impact<br/>d. Retention<br/>e. User Flows|<details><summary>Answer</summary>d. Retention<br/><br/>The retention feature in Azure Application Insights helps you analyze how many users return to your app, and how often they perform particular tasks or achieve goals. For example, if you run a game site, you could compare the numbers of users who return to the site after losing a game with the number who return after winning.<br/>https://docs.microsoft.com/en-us/azure/azure-monitor/app/usage-retention</details>|
|41|You develop an ASP.NET Core MVC application. You need to identify trends in application usage.<br/>You configure the application to track webpages and custom events.<br/>You need to find the places in the application that users often perform repetitive actions.<br/>Which Azure Application Insights Usage Analysis features should you use ?<br/><br/>a. Users<br/>b. Funnels<br/>c. Impact<br/>d. Retention<br/>e. User Flows|<details><summary>Answer</summary>e. User Flows<br/><br/>The User Flows tool visualizes how users navigate between the pages and features of your site. It's great for answering questions like:<br/>•How do users navigate away from a page on your site?<br/>•What do users click on a page on your site?<br/>Where are the places that users churn most from your site?<br/>Are there places where users repeat the same action over and over?<br/>https://docs.microsoft.com/en-us/azure/azure-monitor/app/usage-flows</details>|
|42|You are developing an Azure function that is triggered by an Azure Storage queue.<br/>The Azure function connects to an Azure SQL Database instance.<br/>There are a number of System.InvalidOperationExceptions errors in the error log with the following message:<br/><br/>Timeout expired. The timeout period elapsed prior to obtaining a connection from the pool. This may have occurred because all pooled connections were in use and max pool size was reached.<br/><br/>What should you do to prevent the exception ?<br/><br/>a. Decrease the value of the batchSize option in the host.json file<br/>b. Convert the trigger to Azure Even Hub<br/>c. Change the Azure Function to use Premium plan<br/>d. Change the value of the type option to queueScaling in the function.json file|<details><summary>Answer</summary>a. Decrease the value of the batchSize option in the host.json file<br/><br/>This issue can be fixed in two ways<br/>1. The number of function executions must be according to max pool size of the database<br/>2. Increase the pool size of the database.<br/>By using batch size option, you can configure the number of queue messages that the Functions runtime retrieves simultaneously and processes in parallel. Thus decreasing the load on database<br/>https://docs.microsoft.com/en-us/azure/azure-functions/functions-bindings-storage-queue</details>|
|43|You are developing a food delivery mobile application.<br/><br/>Users can use the application to order from a restaurant in their area<br/><br/>The app uses the following workflow:<br/>1. A driver selects the restaurants from which they will deliver orders.<br/>2. Orders are sent to all available drivers in an area.<br/>3. Only orders for the selected restaurants will appear for the driver.<br/>4. The first driver to accept an order removes it from the list of available orders.<br/><br/>You need to implement an Azure Service Bus solution.<br/>Which three actions should you perform in sequence ?<br/><br/>a. Create a single Service Bus Namespace<br/>Create a single Service Bus topic for each restaurant for which a driver can receive orders<br/>Create a Service Bus subscription for each restaurant for which a driver can receive orders<br/><br/>b. Create a single Service Bus Namespace<br/>Create a single Service Bus topic<br/>Create a Service Bus subscription for each restaurant for which a driver receive orders<br/><br/>c. Crea a single Service Bus Namespace for each restaurant for which a driver can receive orders<br/>Create a single Service Bus topic<br/>Create a Service Bus subscription for each restaurant for which a driver can receive orders|<details><summary>Answer</summary>b. Crea a single Service Bus Namespace<br/>Create a single Service Bus topic<br/>Create a Service Bus subscription for each restaurant for which a driver can receive orders<br/><br/>A namespace is a container for all messaging components. Multiple queues and topics can be in a single namespace, and namespaces often serve as application containers. A namespace can be compared to a "server" in the terminology of other brokers, but the concepts aren't directly equivalent.<br/>Topics and subscriptions. Enable 1:n relationships between publishers and subscribers, allowing subscribers to select particular messages from a published message stream.<br/>In this scenario, drivers subscribe to the restaurants.<br/>https://docs.microsoft.com/en-us/azure/service-bus-messaging/service-bus-messaging-overview</details>|
|44|You have an Azure blob storage that stores images. You have configured role-based access control (RBAC) role permissions on the containers that store images.<br/>You are developing an ASP.NET Core web application that manages images in the Azure Blob Storage containers.<br/>Azure Active Directory (Azure AD) credentials is used for user’s authentication.<br/>You have assigned RBAC roles to users on the Azure Blob Storage containers.<br/>You need to configure the web application’s Azure AD Application so that user permissions can be used with the Azure Blob containers.<br/><br/>How should you configure the Azure Storage API ?<br/><br/>a. Permission:client_id<br/>b. Permission:user_impersonation<br/>c. Permission Type:delegated<br/>d. Permission Type:application|<details><summary>Answer</summary>b. Permission:user_impersonation<br/>c. Permission Type:delegated<br/><br/>To grant your application permissions to call Azure Storage APIs:<br/>On the API permissions page for your registered application, select Add a permission<br/>Under the Microsoft APIs tab, select Azure Storage<br/>On Request API permissions pane, under What type of permissions does your application require?, observe that the available permission type is Delegated permissions. This option is selected for you by default.<br/>Under Permissions, select the checkbox next to user_impersonation, then select the Add permissions button.<br/>https://docs.microsoft.com/en-us/azure/storage/common/storage-auth-aad-app?tabs=dotnet#grant-your-registered-app-permissions-to-azure-storage</details>|
|45|You have an Azure blob storage that stores images. You have configured role-based access control (RBAC) role permissions on the containers that store images.<br/><br/>You are developing an ASP.NET Core web application that manages images in the Azure Blob Storage containers.<br/><br/>Azure Active Directory (Azure AD) credentials is used for user’s authentication.<br/>You have assigned RBAC roles to users on the Azure Blob Storage containers.<br/>You need to configure the web application’s Azure AD Application so that user permissions can be used with the Azure Blob containers.<br/><br/>How should you configure the Microsoft Graph API ?<br/><br/>a. Permission Type:delegated<br/>b. Permission Type:application|<details><summary>Answer</summary>a. Permission Type:delegated<br/><br/>Delegated permissions are appropriate for client apps that access a web API as the signed-in user<br/>https://docs.microsoft.com/en-us/azure/active-directory/develop/quickstart-configure-app-access-web-apis</details>|
|46|You have an Azure storage account which contains two containers.<br/><br/>You need to copy specific blobs across containers in the most efficient manner possible.<br/><br/>What should you do ?<br/><br/>a. Use the Azure PowerShell command Start-AzureStorageBlobCopy<br/>b. Run the following Azure PowerShell command:<br/>AzCopy /Source:$sourceServer /Dest:$DestServer /SourceKey:sourcekey /DestKey:$destKey /S<br/>c. Use storage SDK's CloudFile.StartCopyAsync method|<details><summary>Answer</summary>b. Run the following Azure PowerShell command:<br/>azCopy /Source:$sourceServer /Dest:$DestServer /SourceKey:sourceKey /DestKey:$destKey /S<br/><br/>AzCopy is a command-line utility that you can use to copy blobs or files to or from a storage account. AzCopy is the considered as efficient tool to transfer data.<br/>https://docs.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-v10</details>|
|47|You are developing an app that uses Event Grid.<br/>Your app's event data will be sent to a serverless function that checks compliance.<br/>You write a new event subscription at the scope of your resource.<br/>The event must be invalidated after a specific period of time. You need to configure Event Grid.<br/>What should you implement for WebHook event delivery ?<br/><br/>a. ValidationURL Handshake<br/>b. ValidationCode Handshake<br/>c. JWT token|<details><summary>Answer</summary>b. ValidationCode Handshake<br/><br/>Event Grid requires you to prove ownership of your Webhook endpoint before it starts delivering events to that endpoint. This requirement prevents a malicious user from flooding your endpoint with events. Event Grid sends a subscription validation event to your endpoint. The schema of this event is similar to any other Event Grid event. The data portion of this event includes a validationCode property. Your application verifies that the validation request is for an expected event subscription, and returns the validation code in the response synchronously. This handshake mechanism is supported in all Event Grid versions.<br/>https://docs.microsoft.com/en-us/azure/event-grid/webhook-event-delivery</details>|
|48|You are creating a hazard notification system.<br/>It has a single signaling server which triggers audio and visual alarms to start and stop.<br/>You implement Azure Service Bus to publish alarms.<br/>Each alarm controller uses Azure Service Bus to receive alarm signals as part of a transaction.<br/>Alarm events must be recorded for audit purposes.<br/>Each transaction record must include information about the alarm type that was activated.<br/>You need to implement a reply trail auditing solution.<br/>Which two actions should you perform ?<br/><br/>a. Assign the value of the hazard message MessageId property to the DeliveryCount property<br/>b. Assign the value of the hazard message SequenceNumber property to the DeliveryCount property<br/>c. Assign the value of the hazard message MessageId property to the SequenceNumber property<br/>d. Assign the value of the hazard message MessageId property to the CorrelationId property<br/>e. Assign the value of the hazard message SessionID property to the SequenceNumber property<br/>f. Assign the value of the hazard message SessionID property to the ReplyToSessionId property|<details><summary>Answer</summary>d. Assign the value of the hazard message MessageId property to the CorrelationId property<br/>f. Assign the value of the hazard message SessionID property to the ReplyToSessionId property<br/><br/>CorrelationId (correlation-id) - Enables an application to specify a context for the message for the purposes of correlation; for example, reflecting the MessageId of a message that is being replied to<br/>ReplyToSessionId (reply-to-group-id) - This value augments the ReplyTo information and specifies which SessionId should be set for the reply when sent to the reply entity.<br/>https://docs.microsoft.com/en-us/azure/service-bus-messaging/service-bus-messages-payloads</details>|
|49|You are developing an application using Azure Storage Client library for .NET.<br/>The application contains the following code.<br/>`01 CloudBlockBlob src = null;`<br/>`02 try`<br/>`03 {`<br/>`04  src = container.ListBlobs().OfType<CloudBlockBlob>().FirstOrDefault();`<br/>`05  var id = await src.AcquireLeaseAsync(null);`<br/>`06  var dst = container.GetBlockBlobReference(src.Name);`<br/>`07  string cpid = await dst.StartCopyAsync(src);`<br/>`08  await dst.FetchAttributeAsync();`<br/>`09  return id;`<br/>`}`<br/>`11 catch (Exception e)`<br/>`12 {`<br/>`13  throw;`<br/>`14 }`<br/>`15 finally`<br/>`16 {`<br/>`17  if (src != null)`<br/>`18  await src.FetchAttributeAsync();`<br/>`19  if (src.Properties.LeaseState != LeaseState.Available)`<br/>`20  await src.BreakLeaseAsync(new TimeSpan(0));`<br/>`21 }`<br/><br/>Select Yes if the below statement is true. Otherwise, select No<br/><br/>1. The code segment creates an infinite lease<br/>2. The code always creates a new blob (Refer line 06)<br/>3. The finally block releases the lease<br/><br/>a. Yes,Yes,Yes<br/>b. Yes,No,Yes<br/>c. Yes,No,No<br/>d. No,No,No<br/>e. Yes,Yes,No|<details><summary>Answer</summary>b. Yes,No,No,Yes<br/><br/>1. AcquireLeaseAsync does not specify leaseTime.<br/>leaseTime is a TimeSpan representing the span of time for which to acquire the lease, which will be rounded down to seconds. If null, an infinite lease will be acquired.<br/>2. The GetBlockBlobReference method just gets a reference to a block blob in this container<br/>3. BreakLeaseAsync - breakPeriod parameter - A TimeSpan representing the amount of time to allow the lease to remain, which will be rounded down to seconds. In this case, it is zero. so it releases the lease<br/>https://docs.microsoft.com/en-us/rest/api/storageservices/lease-blob<br/>https://docs.microsoft.com/en-us/dotnet/api/microsoft.azure.storage.blob.cloudblobcontainer.breakleaseasync?view=azure-dotnet-legacy</details>|
|50|You are a developer for HealthEngine Inc., a SaaS company that provides a solution for managing employee expenses. The solution consists of an ASP.NET Core Web API project that is deployed as an Azure Web App.<br/>**Overall architecture -**<br/>Employees upload receipts for the system to process. When processing is complete, the employee receives a summary report email that details the processing results. Employees then use a web application to manage their receipts and perform any additional tasks needed for reimbursement.<br/>**Receipt processing -**<br/>Employees may upload receipts in two ways:<br/>Uploading using an Azure Files mounted folder<br/>Uploading using the web application<br/>**Data Storage -**<br/>Receipt and employee information is stored in an Azure SQL database.<br/>**Documentation -**<br/>Employees are provided with a getting started document when they first use the solution. The documentation includes details on supported operating systems for Azure File upload, and instructions on how to configure the mounted folder.<br/>**Solution details -**<br/>**Users table -**<br/>[<img src="https://i.imgur.com/TgHUZnn.png">](https://i.imgur.com/TgHUZnn.png)<br/>**Web Application -**<br/>You enable MSI for the Web App and configure the Web App to use the security principal name WebAppIdentity.<br/>**Processing -**<br/>Processing is performed by an Azure Function that uses version 2 of the Azure Function runtime. Once processing is completed, results are stored in Azure Blob Storage and an Azure SQL database. Then, an email summary is sent to the user with a link to the processing report. The link to the report must remain valid if the email is forwarded to another user.<br/>**Logging -**<br/>Azure Application Insights is used for telemetry and logging in both the processor and the web application. The processor also has TraceWriter logging enabled.<br/>Application Insights must always contain all log messages.<br/>**Requirements -**<br/>**Receipt processing -**<br/>Concurrent processing of a receipt must be prevented.<br/>**Disaster recovery -**<br/>Regional outage must not impact application availability. All DR operations must not be dependent on application running and must ensure that data in the DR region is up to date.<br/>**Security -**<br/>User's SecurityPin must be stored in such a way that access to the database does not allow the viewing of SecurityPins. The web application is the only system that should have access to SecurityPins.<br/>All certificates and secrets used to secure data must be stored in Azure Key Vault.<br/>You must adhere to the principle of least privilege and provide privileges which are essential to perform the intended function.<br/>All access to Azure Storage and Azure SQL database must use the application's Managed Service Identity (MSI).<br/>Receipt data must always be encrypted at rest.<br/>All data must be protected in transit.<br/>User's expense account number must be visible only to logged in users. All other views of the expense account number should include only the last segment, with the remaining parts obscured.<br/>In the case of a security breach, access to all summary reports must be revoked without impacting other parts of the system.<br/>**Issues -**<br/>**Upload format issue -**<br/>Employees occasionally report an issue with uploading a receipt using the web application. They report that when they upload a receipt using the Azure File Share, the receipt does not appear in their profile. When this occurs, they delete the file in the file share and use the web application, which returns a 500 Internal Server error page.<br/>**Capacity issue -**<br/>During busy periods, employees report long delays between the time they upload the receipt and when it appears in the web application.<br/>**Log capacity issue -**<br/>Developers report that the number of log messages in the trace output for the processor is too high, resulting in lost log messages.<br/>**Application code -**<br/>**Processing.cs -**<br/>`PC01 public static class Processing`<br/>`PC02 {`<br/>`PC03     public static class Function`<br/>`PC04     {`<br/>`PC05         [FunctionName("IssueWork")]`<br/>`PC06         public static async Task run([TimerTrigger("0 */5 * * * *")] TimerInfo timer, ILogger log)`<br/>`PC07         {`<br/>`PC08             var container = await GetCloudBlobContainer();`<br/>`PC09             foreach (var fileItem in await ListFiles())`<br/>`PC10             {`<br/>`PC11                 var file = new CloudFile(fileItem.StorageUri.PrimaryUri);`<br/>`PC12                 var ms = new MemoryStream();`<br/>`PC13                 await file.DownloadToStreamAsync(ms);`<br/>`PC14                 var blob = container.GetBlockblobReference(fileItem.Uri.ToString());`<br/>`PC15                 await blob.UploadFromStreamAsync(ms);`<br/>`PC16             }`<br/>`PC17         }`<br/>`PC18         private static CloudBlockBlob GetDRBlob(CloudBlockBlob sourceBlob)`<br/>`PC19         {`<br/>`PC20             ...`<br/>`PC21         }`<br/>`PC22         private async Task<CloudBlobContainer> GetCloudBlobContainer()`<br/>`PC23         {`<br/>`PC24             var CloudBlobClient = new CloudBlobClient(new Uri("..."), await GetCredentials());`<br/>`PC25`<br/>`PC26             await cloudBlobClient.GetRootContainerReference().CreateIfNotExistAsync();`<br/>`PC27             return cloudBlobClient.GetRootContainerReference();`<br/>`PC28         }`<br/>`PC29         private static async Task<StorageCredentials> GetCredentials()`<br/>`PC30         {`<br/>`PC31             ...`<br/>`PC32         }`<br/>`PC33         private static async Task<List<IListFileItem>> ListFiles()`<br/>`PC34         {`<br/>`            ...`<br/>`PC36         }`<br/>`PC37         private KeyVaultClient _KeyVaultClient = new KeyVaultClient("...");`<br/>`PC38     }`<br/>`PC39 }`<br/><br/>**Database.cs -**<br/>`DB01 public class Database`<br/>`DB02 {`<br/>`DB03     private string ConnectionString =`<br/>`DB04 `<br/>`DB05     public async Task<object> LoadUserDetails(string userId)`<br/>`DB06     {`<br/>`DB07 `<br/>`DB08             return await policy.ExecuteAsync(async () =>`<br/>`DB09             {`<br/>`DB10                 using (var connection = new SqlConnection(ConnectionString))`<br/>`DB11                 {`<br/>`DB12                     await connection.OpenAsync();`<br/>`DB13                     using (var command = new SqlCommand("...", connection))`<br/>`DB14                     using (var reader = command.ExecuteReader())`<br/>`DB15                     {`<br/>`DB16                         ...`<br/>`DB17                     }`<br/>`DB18                 }`<br/>`DB19             });`<br/>`DB20     }`<br/>`DB21 }`<br/>**ReceiptUploader.cs -**<br/>`RU01 public class ReceipUploader`<br/>`RU02 {`<br/>`RU03     public async Task UploadFile(string file, byte[] binary)`<br/>`RU04     {`<br/>`RU05         var httpClient = new HttpClient();`<br/>`RU06         var response = await httpClient.PutAsync("...", new ByteArrayContent(binary));`<br/>`RU07         while (ShouldRetry(response))`<br/>`RU08         {`<br/>`RU09             response = await httpClient.PutAsync("...", new ByteArrayContent(binary));`<br/>`RU10         }`<br/>`RU11     }`<br/>`RU12     private bool ShouldRetry(HttpReponseMessage response)`<br/>`RU13     {`<br/>`RU14`<br/>`RU15     }`<br/>`RU16 }`<br/>**ConfigureSSE.ps1 -**<br/>`CS01 $storageAccount = Gt-AzureRmStorageAccount -ResourceGroupName "..." -AccountName "..."`<br/>`CS02 $keyVault = Get-AzureRmKeyVault -VaultName "..."`<br/>`CS03 $key = Get-AzureKeyVaultKey -VaultName $keyVault.VaultName -Name "..."`<br/>`Set-AzureRmKeyVaultAccessPolicy ``\`<br/>`CS05    -VaultName $keyVault.VaultName ``\`<br/>`CS06     -ObjectId $storageAccount.Identity.PrincipalId ``\`<br/>`CS07`<br/>`CS08`<br/>`CS09 set-AzureRmStorageAccount ``\`<br/>`CS10     -ResourceGroupName $storageAccount.ResourceGroupName ``\`<br/>`CS11     -AccountName $storageAccount.StorageAccountName ``\`<br/>`CS12     -EnableEncryptionService File ``\`<br/>`CS13     -KeyvaultEncryption ``\`<br/>`CS14     -KeyName $key.Name ``\`<br/>`CS15     -KeyVersion $key.Version ``\`<br/>`CS16     -KeyVaultUri $keyVault.VaultUri`<br/>**Question**<br/>You need to add code at line PC26 of Processing.cs to ensure that security policies are met.<br/>How should you complete the code that you will add at line PC26?<br/>`var resolver = new KeyVaultKeyResolver(_keyVaultClient);`<br/>`var keybundle = await _keyVaultClient.GetKeyAsync("...","...");`<br/>`<Code Snippet1>`<br/>`<Code Snippet2>`<br/>`<Code Snippet3>`<br/><br/>a. Code Snippet1:var key = keyBundle.Key;<br/>b. Code Snippet1:var key = keyBundle.KeyIdentifier.Identifier;<br/>c. Code Snippet:var key = await resolver.ResolveKeyAsync("encrypt",null);d. Code Snippet:var key = await<br/>resolver.ResolveKeyAsync(keyBundle.KeyIdentifier.Identifier,CancellationToken.None);<br/>e. Code Snippet2:var x = keyBundle.Managed<br/>f. Code Snippet2:var x = AuthenticationScheme.SharedKey;<br/>g. Code Snippet2: var x = new BlobEncryptionPolicy(key,resolver);<br/>h. Code Snippet2:var x = new DeleteRetentionPolicy{Enabled = key.Kid != null};<br/>i. Code Snippet3:cloudBlobClient.AuthenticationScheme = x;<br/>j. Code Snippet3:cloudBlobClient.DefaultRequestOptions.RequireEncryption = x;<br/>k . Code Snippet3:cloudBlobClient.DefaultRequestOptions.EncryptionPolicy = x;<br/>l. Code Snippet3:cloudblobClient.SetServiceProperties(new ServiceProperties(deleteRetentionPolicy:x));|<details><summary>Answer</summary>d. Code Snippet1: var key = await resolver.ResolverKeyAsync(keyBundle.KeyIdentifier.Identifier, CancellationToken.None);<br/>h. Code Snippet2: var x = new BlobEncryptionPolicy(key,resolver);<br/>l. Code Snippet3:cloudBlobClient.DefaultRequestOptions.EncryptionPolicy =x;<br/><br/>The requirement from security policy is to encrypt data in transit and at rest<br/>Below is the sample code to encrypt a blob and upload<br/>// Retrieve the key that you created previously.<br/>// The IKey that is returned here is an RsaKey.<br/>var rsa = cloudResolver.ResolveKeyAsync(<br/>"https://contosokeyvault.vault.azure.net/keys/TestRSAKey1",<br/>CancellationToken.None).GetAwaiter().GetResult();<br/>// Now you simply use the RSA key to encrypt by setting it in the BlobEncryptionPolicy.<br/>BlobEncryptionPolicy policy = new BlobEncryptionPolicy(rsa, null);<br/>BlobRequestOptions options = new BlobRequestOptions() { EncryptionPolicy = policy };<br/><br/>// Reference a block blob.<br/>CloudBlockBlob blob = contain.GetBlockBlobReference("MyFile.txt");<br/><br/>// Upload using the UploadFromStream method.<br/>using (var stream = System.IO.File.OpenRead(@"C:\Temp\MyFile.txt"))<br/>blob.UploadFromStream(stream, stream.Length, null, options, null);</details>|
|51|You are a developer for HealthEngine Inc., a SaaS company that provides a solution for managing employee expenses. The solution consists of an ASP.NET Core Web API project that is deployed as an Azure Web App.<br/>**Overall architecture -**<br/>Employees upload receipts for the system to process. When processing is complete, the employee receives a summary report email that details the processing results. Employees then use a web application to manage their receipts and perform any additional tasks needed for reimbursement.<br/>**Receipt processing -**<br/>Employees may upload receipts in two ways:<br/>Uploading using an Azure Files mounted folder<br/>Uploading using the web application<br/>**Data Storage -**<br/>Receipt and employee information is stored in an Azure SQL database.<br/>**Documentation -**<br/>Employees are provided with a getting started document when they first use the solution. The documentation includes details on supported operating systems for Azure File upload, and instructions on how to configure the mounted folder.<br/>**Solution details -**<br/>**Users table -**<br/>[<img src="https://i.imgur.com/TgHUZnn.png">](https://i.imgur.com/TgHUZnn.png)<br/>**Web Application -**<br/>You enable MSI for the Web App and configure the Web App to use the security principal name WebAppIdentity.<br/>**Processing -**<br/>Processing is performed by an Azure Function that uses version 2 of the Azure Function runtime. Once processing is completed, results are stored in Azure Blob Storage and an Azure SQL database. Then, an email summary is sent to the user with a link to the processing report. The link to the report must remain valid if the email is forwarded to another user.<br/>**Logging -**<br/>Azure Application Insights is used for telemetry and logging in both the processor and the web application. The processor also has TraceWriter logging enabled.<br/>Application Insights must always contain all log messages.<br/>**Requirements -**<br/>**Receipt processing -**<br/>Concurrent processing of a receipt must be prevented.<br/>**Disaster recovery -**<br/>Regional outage must not impact application availability. All DR operations must not be dependent on application running and must ensure that data in the DR region is up to date.<br/>**Security -**<br/>User's SecurityPin must be stored in such a way that access to the database does not allow the viewing of SecurityPins. The web application is the only system that should have access to SecurityPins.<br/>All certificates and secrets used to secure data must be stored in Azure Key Vault.<br/>You must adhere to the principle of least privilege and provide privileges which are essential to perform the intended function.<br/>All access to Azure Storage and Azure SQL database must use the application's Managed Service Identity (MSI).<br/>Receipt data must always be encrypted at rest.<br/>All data must be protected in transit.<br/>User's expense account number must be visible only to logged in users. All other views of the expense account number should include only the last segment, with the remaining parts obscured.<br/>In the case of a security breach, access to all summary reports must be revoked without impacting other parts of the system.<br/>**Issues -**<br/>**Upload format issue -**<br/>Employees occasionally report an issue with uploading a receipt using the web application. They report that when they upload a receipt using the Azure File Share, the receipt does not appear in their profile. When this occurs, they delete the file in the file share and use the web application, which returns a 500 Internal Server error page.<br/>**Capacity issue -**<br/>During busy periods, employees report long delays between the time they upload the receipt and when it appears in the web application.<br/>**Log capacity issue -**<br/>Developers report that the number of log messages in the trace output for the processor is too high, resulting in lost log messages.<br/>**Application code -**<br/>**Processing.cs -**<br/>`PC01 public static class Processing`<br/>`PC02 {`<br/>`PC03     public static class Function`<br/>`PC04     {`<br/>`PC05         [FunctionName("IssueWork")]`<br/>`PC06         public static async Task run([TimerTrigger("0 */5 * * * *")] TimerInfo timer, ILogger log)`<br/>`PC07         {`<br/>`PC08             var container = await GetCloudBlobContainer();`<br/>`PC09             foreach (var fileItem in await ListFiles())`<br/>`PC10             {`<br/>`PC11                 var file = new CloudFile(fileItem.StorageUri.PrimaryUri);`<br/>`PC12                 var ms = new MemoryStream();`<br/>`PC13                 await file.DownloadToStreamAsync(ms);`<br/>`PC14                 var blob = container.GetBlockblobReference(fileItem.Uri.ToString());`<br/>`PC15                 await blob.UploadFromStreamAsync(ms);`<br/>`PC16             }`<br/>`PC17         }`<br/>`PC18         private static CloudBlockBlob GetDRBlob(CloudBlockBlob sourceBlob)`<br/>`PC19         {`<br/>`PC20             ...`<br/>`PC21         }`<br/>`PC22         private async Task<CloudBlobContainer> GetCloudBlobContainer()`<br/>`PC23         {`<br/>`PC24             var CloudBlobClient = new CloudBlobClient(new Uri("..."), await GetCredentials());`<br/>`PC25`<br/>`PC26             await cloudBlobClient.GetRootContainerReference().CreateIfNotExistAsync();`<br/>`PC27             return cloudBlobClient.GetRootContainerReference();`<br/>`PC28         }`<br/>`PC29         private static async Task<StorageCredentials> GetCredentials()`<br/>`PC30         {`<br/>`PC31             ...`<br/>`PC32         }`<br/>`PC33         private static async Task<List<IListFileItem>> ListFiles()`<br/>`PC34         {`<br/>`            ...`<br/>`PC36         }`<br/>`PC37         private KeyVaultClient _KeyVaultClient = new KeyVaultClient("...");`<br/>`PC38     }`<br/>`PC39 }`<br/><br/>**Database.cs -**<br/>`DB01 public class Database`<br/>`DB02 {`<br/>`DB03     private string ConnectionString =`<br/>`DB04 `<br/>`DB05     public async Task<object> LoadUserDetails(string userId)`<br/>`DB06     {`<br/>`DB07 `<br/>`DB08             return await policy.ExecuteAsync(async () =>`<br/>`DB09             {`<br/>`DB10                 using (var connection = new SqlConnection(ConnectionString))`<br/>`DB11                 {`<br/>`DB12                     await connection.OpenAsync();`<br/>`DB13                     using (var command = new SqlCommand("...", connection))`<br/>`DB14                     using (var reader = command.ExecuteReader())`<br/>`DB15                     {`<br/>`DB16                         ...`<br/>`DB17                     }`<br/>`DB18                 }`<br/>`DB19             });`<br/>`DB20     }`<br/>`DB21 }`<br/>**ReceiptUploader.cs -**<br/>`RU01 public class ReceipUploader`<br/>`RU02 {`<br/>`RU03     public async Task UploadFile(string file, byte[] binary)`<br/>`RU04     {`<br/>`RU05         var httpClient = new HttpClient();`<br/>`RU06         var response = await httpClient.PutAsync("...", new ByteArrayContent(binary));`<br/>`RU07         while (ShouldRetry(response))`<br/>`RU08         {`<br/>`RU09             response = await httpClient.PutAsync("...", new ByteArrayContent(binary));`<br/>`RU10         }`<br/>`RU11     }`<br/>`RU12     private bool ShouldRetry(HttpReponseMessage response)`<br/>`RU13     {`<br/>`RU14`<br/>`RU15     }`<br/>`RU16 }`<br/>**ConfigureSSE.ps1 -**<br/>`CS01 $storageAccount = Gt-AzureRmStorageAccount -ResourceGroupName "..." -AccountName "..."`<br/>`CS02 $keyVault = Get-AzureRmKeyVault -VaultName "..."`<br/>`CS03 $key = Get-AzureKeyVaultKey -VaultName $keyVault.VaultName -Name "..."`<br/>`Set-AzureRmKeyVaultAccessPolicy ``\`<br/>`CS05    -VaultName $keyVault.VaultName ``\`<br/>`CS06     -ObjectId $storageAccount.Identity.PrincipalId ``\`<br/>`CS07`<br/>`CS08`<br/>`CS09 set-AzureRmStorageAccount ``\`<br/>`CS10     -ResourceGroupName $storageAccount.ResourceGroupName ``\`<br/>`CS11     -AccountName $storageAccount.StorageAccountName ``\`<br/>`CS12     -EnableEncryptionService File ``\`<br/>`CS13     -KeyvaultEncryption ``\`<br/>`CS14     -KeyName $key.Name ``\`<br/>`CS15     -KeyVersion $key.Version ``\`<br/>`CS16     -KeyVaultUri $keyVault.VaultUri`<br/>**Question**<br/>You need to ensure the security policies are met.<br/>What code do you add at line CS07 of ConfigureSSE.ps1 ?<br/><br/>a. PermissionstoKeys. create,encrypt,decrypt<br/>b. PermissionsToCertificates create,encrypt,decrypt<br/>c. PermissionsToCertificates wrapkey,unwrapkey,get<br/>d. PermissionsToKeys wrapkey,unwrapkey,get|<details><summary>Answer</summary>d. PermissionsToKeys wrapkey,unwrapkey,get<br/><br/>All certificates and secrets used to secure data must be stored in Azure Key Vault<br/>You need to retrieve the keys so get permission is required. The wrapkey and unwrapkey will be used for symmetric encryption to encrypt the blobs.<br/>Below link contains an example of same scenario<br/>https://docs.microsoft.com/en-us/powershell/module/az.storage/set-azstorageaccount?view=azps-8.0.0#example-5-set-encryption-keysource-to-keyvault<br/>https://docs.microsoft.com/en-us/azure/key-vault/keys/about-keys-details#key-access-control</details>|
|52|You are a developer for HealthEngine Inc., a SaaS company that provides a solution for managing employee expenses. The solution consists of an ASP.NET Core Web API project that is deployed as an Azure Web App.<br/>**Overall architecture -**<br/>Employees upload receipts for the system to process. When processing is complete, the employee receives a summary report email that details the processing results. Employees then use a web application to manage their receipts and perform any additional tasks needed for reimbursement.<br/>**Receipt processing -**<br/>Employees may upload receipts in two ways:<br/>Uploading using an Azure Files mounted folder<br/>Uploading using the web application<br/>**Data Storage -**<br/>Receipt and employee information is stored in an Azure SQL database.<br/>**Documentation -**<br/>Employees are provided with a getting started document when they first use the solution. The documentation includes details on supported operating systems for Azure File upload, and instructions on how to configure the mounted folder.<br/>**Solution details -**<br/>**Users table -**<br/>[<img src="https://i.imgur.com/TgHUZnn.png">](https://i.imgur.com/TgHUZnn.png)<br/>**Web Application -**<br/>You enable MSI for the Web App and configure the Web App to use the security principal name WebAppIdentity.<br/>**Processing -**<br/>Processing is performed by an Azure Function that uses version 2 of the Azure Function runtime. Once processing is completed, results are stored in Azure Blob Storage and an Azure SQL database. Then, an email summary is sent to the user with a link to the processing report. The link to the report must remain valid if the email is forwarded to another user.<br/>**Logging -**<br/>Azure Application Insights is used for telemetry and logging in both the processor and the web application. The processor also has TraceWriter logging enabled.<br/>Application Insights must always contain all log messages.<br/>**Requirements -**<br/>**Receipt processing -**<br/>Concurrent processing of a receipt must be prevented.<br/>**Disaster recovery -**<br/>Regional outage must not impact application availability. All DR operations must not be dependent on application running and must ensure that data in the DR region is up to date.<br/>**Security -**<br/>User's SecurityPin must be stored in such a way that access to the database does not allow the viewing of SecurityPins. The web application is the only system that should have access to SecurityPins.<br/>All certificates and secrets used to secure data must be stored in Azure Key Vault.<br/>You must adhere to the principle of least privilege and provide privileges which are essential to perform the intended function.<br/>All access to Azure Storage and Azure SQL database must use the application's Managed Service Identity (MSI).<br/>Receipt data must always be encrypted at rest.<br/>All data must be protected in transit.<br/>User's expense account number must be visible only to logged in users. All other views of the expense account number should include only the last segment, with the remaining parts obscured.<br/>In the case of a security breach, access to all summary reports must be revoked without impacting other parts of the system.<br/>**Issues -**<br/>**Upload format issue -**<br/>Employees occasionally report an issue with uploading a receipt using the web application. They report that when they upload a receipt using the Azure File Share, the receipt does not appear in their profile. When this occurs, they delete the file in the file share and use the web application, which returns a 500 Internal Server error page.<br/>**Capacity issue -**<br/>During busy periods, employees report long delays between the time they upload the receipt and when it appears in the web application.<br/>**Log capacity issue -**<br/>Developers report that the number of log messages in the trace output for the processor is too high, resulting in lost log messages.<br/>**Application code -**<br/>**Processing.cs -**<br/>`PC01 public static class Processing`<br/>`PC02 {`<br/>`PC03     public static class Function`<br/>`PC04     {`<br/>`PC05         [FunctionName("IssueWork")]`<br/>`PC06         public static async Task run([TimerTrigger("0 */5 * * * *")] TimerInfo timer, ILogger log)`<br/>`PC07         {`<br/>`PC08             var container = await GetCloudBlobContainer();`<br/>`PC09             foreach (var fileItem in await ListFiles())`<br/>`PC10             {`<br/>`PC11                 var file = new CloudFile(fileItem.StorageUri.PrimaryUri);`<br/>`PC12                 var ms = new MemoryStream();`<br/>`PC13                 await file.DownloadToStreamAsync(ms);`<br/>`PC14                 var blob = container.GetBlockblobReference(fileItem.Uri.ToString());`<br/>`PC15                 await blob.UploadFromStreamAsync(ms);`<br/>`PC16             }`<br/>`PC17         }`<br/>`PC18         private static CloudBlockBlob GetDRBlob(CloudBlockBlob sourceBlob)`<br/>`PC19         {`<br/>`PC20             ...`<br/>`PC21         }`<br/>`PC22         private async Task<CloudBlobContainer> GetCloudBlobContainer()`<br/>`PC23         {`<br/>`PC24             var CloudBlobClient = new CloudBlobClient(new Uri("..."), await GetCredentials());`<br/>`PC25`<br/>`PC26             await cloudBlobClient.GetRootContainerReference().CreateIfNotExistAsync();`<br/>`PC27             return cloudBlobClient.GetRootContainerReference();`<br/>`PC28         }`<br/>`PC29         private static async Task<StorageCredentials> GetCredentials()`<br/>`PC30         {`<br/>`PC31             ...`<br/>`PC32         }`<br/>`PC33         private static async Task<List<IListFileItem>> ListFiles()`<br/>`PC34         {`<br/>`            ...`<br/>`PC36         }`<br/>`PC37         private KeyVaultClient _KeyVaultClient = new KeyVaultClient("...");`<br/>`PC38     }`<br/>`PC39 }`<br/><br/>**Database.cs -**<br/>`DB01 public class Database`<br/>`DB02 {`<br/>`DB03     private string ConnectionString =`<br/>`DB04 `<br/>`DB05     public async Task<object> LoadUserDetails(string userId)`<br/>`DB06     {`<br/>`DB07 `<br/>`DB08             return await policy.ExecuteAsync(async () =>`<br/>`DB09             {`<br/>`DB10                 using (var connection = new SqlConnection(ConnectionString))`<br/>`DB11                 {`<br/>`DB12                     await connection.OpenAsync();`<br/>`DB13                     using (var command = new SqlCommand("...", connection))`<br/>`DB14                     using (var reader = command.ExecuteReader())`<br/>`DB15                     {`<br/>`DB16                         ...`<br/>`DB17                     }`<br/>`DB18                 }`<br/>`DB19             });`<br/>`DB20     }`<br/>`DB21 }`<br/>**ReceiptUploader.cs -**<br/>`RU01 public class ReceipUploader`<br/>`RU02 {`<br/>`RU03     public async Task UploadFile(string file, byte[] binary)`<br/>`RU04     {`<br/>`RU05         var httpClient = new HttpClient();`<br/>`RU06         var response = await httpClient.PutAsync("...", new ByteArrayContent(binary));`<br/>`RU07         while (ShouldRetry(response))`<br/>`RU08         {`<br/>`RU09             response = await httpClient.PutAsync("...", new ByteArrayContent(binary));`<br/>`RU10         }`<br/>`RU11     }`<br/>`RU12     private bool ShouldRetry(HttpReponseMessage response)`<br/>`RU13     {`<br/>`RU14`<br/>`RU15     }`<br/>`RU16 }`<br/>**ConfigureSSE.ps1 -**<br/>`CS01 $storageAccount = Gt-AzureRmStorageAccount -ResourceGroupName "..." -AccountName "..."`<br/>`CS02 $keyVault = Get-AzureRmKeyVault -VaultName "..."`<br/>`CS03 $key = Get-AzureKeyVaultKey -VaultName $keyVault.VaultName -Name "..."`<br/>`Set-AzureRmKeyVaultAccessPolicy ``\`<br/>`CS05    -VaultName $keyVault.VaultName ``\`<br/>`CS06     -ObjectId $storageAccount.Identity.PrincipalId ``\`<br/>`CS07`<br/>`CS08`<br/>`CS09 set-AzureRmStorageAccount ``\`<br/>`CS10     -ResourceGroupName $storageAccount.ResourceGroupName ``\`<br/>`CS11     -AccountName $storageAccount.StorageAccountName ``\`<br/>`CS12     -EnableEncryptionService File ``\`<br/>`CS13     -KeyvaultEncryption ``\`<br/>`CS14     -KeyName $key.Name ``\`<br/>`CS15     -KeyVersion $key.Version ``\`<br/>`CS16     -KeyVaultUri $keyVault.VaultUri`<br/>**Question**<br/>You need to ensure receipt processing occurs correctly.<br/>What should you do ?<br/><br/>a. Use blob properties to prevent concurrency problems<br/>b. Use blob SnapshotTime to prevent concurrency problems<br/>c. Use blob metadata to prevent concurrency problems<br/>d. Use blob leases to prevent concurrency problems|<details><summary>Answer</summary>d. Use blob leases to prevent concurrency problems<br/><br/>Scenario: Processing is performed by an Azure Function that uses version 2 of the Azure Function runtime. Once processing is completed, results are stored in Azure Blob<br/>Storage and an Azure SQL database. Then, an email summary is sent to the user with a link to the processing report. The link to the report must remain valid if the email is forwarded to another user. Concurrent processing of a receipt must be prevented.<br/><br/>Modern applications often have multiple users viewing and updating data simultaneously. Application developers need to think carefully about how to provide a predictable experience to their end users, particularly for scenarios where multiple users can update the same data<br/>To lock a blob for exclusive use, you can acquire a lease on it. When you acquire the lease, you specify the duration of the lease. A finite lease may be valid from between 15 to 60 seconds. A lease can also be infinite, which amounts to an exclusive lock. You can renew a finite lease to extend it, and you can release the lease when you're finished with it. Azure Storage automatically releases finite leases when they expire.<br/>https://docs.microsoft.com/en-us/azure/storage/blobs/concurrency-manage?tabs=dotnet</details>|
|53|You are a developer for HealthEngine Inc., a SaaS company that provides a solution for managing employee expenses. The solution consists of an ASP.NET Core Web API project that is deployed as an Azure Web App.<br/>**Overall architecture -**<br/>Employees upload receipts for the system to process. When processing is complete, the employee receives a summary report email that details the processing results. Employees then use a web application to manage their receipts and perform any additional tasks needed for reimbursement.<br/>**Receipt processing -**<br/>Employees may upload receipts in two ways:<br/>Uploading using an Azure Files mounted folder<br/>Uploading using the web application<br/>**Data Storage -**<br/>Receipt and employee information is stored in an Azure SQL database.<br/>**Documentation -**<br/>Employees are provided with a getting started document when they first use the solution. The documentation includes details on supported operating systems for Azure File upload, and instructions on how to configure the mounted folder.<br/>**Solution details -**<br/>**Users table -**<br/>[<img src="https://i.imgur.com/TgHUZnn.png">](https://i.imgur.com/TgHUZnn.png)<br/>**Web Application -**<br/>You enable MSI for the Web App and configure the Web App to use the security principal name WebAppIdentity.<br/>**Processing -**<br/>Processing is performed by an Azure Function that uses version 2 of the Azure Function runtime. Once processing is completed, results are stored in Azure Blob Storage and an Azure SQL database. Then, an email summary is sent to the user with a link to the processing report. The link to the report must remain valid if the email is forwarded to another user.<br/>**Logging -**<br/>Azure Application Insights is used for telemetry and logging in both the processor and the web application. The processor also has TraceWriter logging enabled.<br/>Application Insights must always contain all log messages.<br/>**Requirements -**<br/>**Receipt processing -**<br/>Concurrent processing of a receipt must be prevented.<br/>**Disaster recovery -**<br/>Regional outage must not impact application availability. All DR operations must not be dependent on application running and must ensure that data in the DR region is up to date.<br/>**Security -**<br/>User's SecurityPin must be stored in such a way that access to the database does not allow the viewing of SecurityPins. The web application is the only system that should have access to SecurityPins.<br/>All certificates and secrets used to secure data must be stored in Azure Key Vault.<br/>You must adhere to the principle of least privilege and provide privileges which are essential to perform the intended function.<br/>All access to Azure Storage and Azure SQL database must use the application's Managed Service Identity (MSI).<br/>Receipt data must always be encrypted at rest.<br/>All data must be protected in transit.<br/>User's expense account number must be visible only to logged in users. All other views of the expense account number should include only the last segment, with the remaining parts obscured.<br/>In the case of a security breach, access to all summary reports must be revoked without impacting other parts of the system.<br/>**Issues -**<br/>**Upload format issue -**<br/>Employees occasionally report an issue with uploading a receipt using the web application. They report that when they upload a receipt using the Azure File Share, the receipt does not appear in their profile. When this occurs, they delete the file in the file share and use the web application, which returns a 500 Internal Server error page.<br/>**Capacity issue -**<br/>During busy periods, employees report long delays between the time they upload the receipt and when it appears in the web application.<br/>**Log capacity issue -**<br/>Developers report that the number of log messages in the trace output for the processor is too high, resulting in lost log messages.<br/>**Application code -**<br/>**Processing.cs -**<br/>`PC01 public static class Processing`<br/>`PC02 {`<br/>`PC03     public static class Function`<br/>`PC04     {`<br/>`PC05         [FunctionName("IssueWork")]`<br/>`PC06         public static async Task run([TimerTrigger("0 */5 * * * *")] TimerInfo timer, ILogger log)`<br/>`PC07         {`<br/>`PC08             var container = await GetCloudBlobContainer();`<br/>`PC09             foreach (var fileItem in await ListFiles())`<br/>`PC10             {`<br/>`PC11                 var file = new CloudFile(fileItem.StorageUri.PrimaryUri);`<br/>`PC12                 var ms = new MemoryStream();`<br/>`PC13                 await file.DownloadToStreamAsync(ms);`<br/>`PC14                 var blob = container.GetBlockblobReference(fileItem.Uri.ToString());`<br/>`PC15                 await blob.UploadFromStreamAsync(ms);`<br/>`PC16             }`<br/>`PC17         }`<br/>`PC18         private static CloudBlockBlob GetDRBlob(CloudBlockBlob sourceBlob)`<br/>`PC19         {`<br/>`PC20             ...`<br/>`PC21         }`<br/>`PC22         private async Task<CloudBlobContainer> GetCloudBlobContainer()`<br/>`PC23         {`<br/>`PC24             var CloudBlobClient = new CloudBlobClient(new Uri("..."), await GetCredentials());`<br/>`PC25`<br/>`PC26             await cloudBlobClient.GetRootContainerReference().CreateIfNotExistAsync();`<br/>`PC27             return cloudBlobClient.GetRootContainerReference();`<br/>`PC28         }`<br/>`PC29         private static async Task<StorageCredentials> GetCredentials()`<br/>`PC30         {`<br/>`PC31             ...`<br/>`PC32         }`<br/>`PC33         private static async Task<List<IListFileItem>> ListFiles()`<br/>`PC34         {`<br/>`            ...`<br/>`PC36         }`<br/>`PC37         private KeyVaultClient _KeyVaultClient = new KeyVaultClient("...");`<br/>`PC38     }`<br/>`PC39 }`<br/><br/>**Database.cs -**<br/>`DB01 public class Database`<br/>`DB02 {`<br/>`DB03     private string ConnectionString =`<br/>`DB04 `<br/>`DB05     public async Task<object> LoadUserDetails(string userId)`<br/>`DB06     {`<br/>`DB07 `<br/>`DB08             return await policy.ExecuteAsync(async () =>`<br/>`DB09             {`<br/>`DB10                 using (var connection = new SqlConnection(ConnectionString))`<br/>`DB11                 {`<br/>`DB12                     await connection.OpenAsync();`<br/>`DB13                     using (var command = new SqlCommand("...", connection))`<br/>`DB14                     using (var reader = command.ExecuteReader())`<br/>`DB15                     {`<br/>`DB16                         ...`<br/>`DB17                     }`<br/>`DB18                 }`<br/>`DB19             });`<br/>`DB20     }`<br/>`DB21 }`<br/>**ReceiptUploader.cs -**<br/>`RU01 public class ReceipUploader`<br/>`RU02 {`<br/>`RU03     public async Task UploadFile(string file, byte[] binary)`<br/>`RU04     {`<br/>`RU05         var httpClient = new HttpClient();`<br/>`RU06         var response = await httpClient.PutAsync("...", new ByteArrayContent(binary));`<br/>`RU07         while (ShouldRetry(response))`<br/>`RU08         {`<br/>`RU09             response = await httpClient.PutAsync("...", new ByteArrayContent(binary));`<br/>`RU10         }`<br/>`RU11     }`<br/>`RU12     private bool ShouldRetry(HttpReponseMessage response)`<br/>`RU13     {`<br/>`RU14`<br/>`RU15     }`<br/>`RU16 }`<br/>**ConfigureSSE.ps1 -**<br/>`CS01 $storageAccount = Gt-AzureRmStorageAccount -ResourceGroupName "..." -AccountName "..."`<br/>`CS02 $keyVault = Get-AzureRmKeyVault -VaultName "..."`<br/>`CS03 $key = Get-AzureKeyVaultKey -VaultName $keyVault.VaultName -Name "..."`<br/>`Set-AzureRmKeyVaultAccessPolicy ``\`<br/>`CS05    -VaultName $keyVault.VaultName ``\`<br/>`CS06     -ObjectId $storageAccount.Identity.PrincipalId ``\`<br/>`CS07`<br/>`CS08`<br/>`CS09 set-AzureRmStorageAccount ``\`<br/>`CS10     -ResourceGroupName $storageAccount.ResourceGroupName ``\`<br/>`CS11     -AccountName $storageAccount.StorageAccountName ``\`<br/>`CS12     -EnableEncryptionService File ``\`<br/>`CS13     -KeyvaultEncryption ``\`<br/>`CS14     -KeyName $key.Name ``\`<br/>`CS15     -KeyVersion $key.Version ``\`<br/>`CS16     -KeyVaultUri $keyVault.VaultUri`<br/>**Question**<br/>You need to add code at line PC32 in Processing.cs to implement the GetCredentials method in the Processing class.<br/>How should you complete the code ?<br/>`var tp = new <Code Block1>`<br/>`var t = new TokenCredential(await <Code Block2> );`<br/>`return new StorageCredentials(t);`<br/><br/>a. Code Block1:AzureServiceTokenProvider()<br/>b. Code Block1:MSITokenProvider("...",null)<br/>c. Code Block1:StringTokenProvider("storage","msi")<br/>d. Code Block2:tp.getAccessTokenAsync("...")<br/>e. Code Block2:tp.getauthenticationHeaderSync(CancellationToken.None)|<details><summary>Answer</summary>a. Code Block1:AzureServiceTokenProvider()<br/>d. CodeBlock2:tp.getAccessTokenAsync("...")<br/><br/>Here is the sample code to connect to Azure SQL.<br/>using Microsoft.Azure.Services.AppAuthentication;<br/>using Microsoft.Azure.KeyVault;<br/>using System.Data.SqlClient<br/><br/>// Use AzureServiceTokenProvider’s built-in callback for KeyVaultClient<br/>var azureServiceTokenProvider = new AzureServiceTokenProvider();<br/>var kv = new KeyVaultClient(new KeyVaultClient.AuthenticationCallback(azureServiceTokenProvider.KeyVaultTokenCallback));<br/><br/><br/>// Request an access token for SqlConnection<br/>sqlConnection = new SqlConnection(YourConnectionString))<br/>{<br/>sqlConnection.AccessToken = await azureServiceTokenProvider.GetAccessTokenAsync("https://database.windows.net");<br/>sqlConnection.Open();<br/>}<br/><br/>Use AzureServiceTokenProvider to simplify requesting access tokens for your Azure clients<br/>The thread-safe AzureServiceTokenProvider class caches the token in memory and retrieves it from Azure AD just before expiration. That means you never need to check the expiration of the token before calling the GetAccessTokenAsync method.<br/>https://docs.microsoft.com/en-us/dotnet/api/overview/azure/service-to-service-authentication</details>|
|54|You are a developer for HealthEngine Inc., a SaaS company that provides a solution for managing employee expenses. The solution consists of an ASP.NET Core Web API project that is deployed as an Azure Web App.<br/>**Overall architecture -**<br/>Employees upload receipts for the system to process. When processing is complete, the employee receives a summary report email that details the processing results. Employees then use a web application to manage their receipts and perform any additional tasks needed for reimbursement.<br/>**Receipt processing -**<br/>Employees may upload receipts in two ways:<br/>Uploading using an Azure Files mounted folder<br/>Uploading using the web application<br/>**Data Storage -**<br/>Receipt and employee information is stored in an Azure SQL database.<br/>**Documentation -**<br/>Employees are provided with a getting started document when they first use the solution. The documentation includes details on supported operating systems for Azure File upload, and instructions on how to configure the mounted folder.<br/>**Solution details -**<br/>**Users table -**<br/>[<img src="https://i.imgur.com/TgHUZnn.png">](https://i.imgur.com/TgHUZnn.png)<br/>**Web Application -**<br/>You enable MSI for the Web App and configure the Web App to use the security principal name WebAppIdentity.<br/>**Processing -**<br/>Processing is performed by an Azure Function that uses version 2 of the Azure Function runtime. Once processing is completed, results are stored in Azure Blob Storage and an Azure SQL database. Then, an email summary is sent to the user with a link to the processing report. The link to the report must remain valid if the email is forwarded to another user.<br/>**Logging -**<br/>Azure Application Insights is used for telemetry and logging in both the processor and the web application. The processor also has TraceWriter logging enabled.<br/>Application Insights must always contain all log messages.<br/>**Requirements -**<br/>**Receipt processing -**<br/>Concurrent processing of a receipt must be prevented.<br/>**Disaster recovery -**<br/>Regional outage must not impact application availability. All DR operations must not be dependent on application running and must ensure that data in the DR region is up to date.<br/>**Security -**<br/>User's SecurityPin must be stored in such a way that access to the database does not allow the viewing of SecurityPins. The web application is the only system that should have access to SecurityPins.<br/>All certificates and secrets used to secure data must be stored in Azure Key Vault.<br/>You must adhere to the principle of least privilege and provide privileges which are essential to perform the intended function.<br/>All access to Azure Storage and Azure SQL database must use the application's Managed Service Identity (MSI).<br/>Receipt data must always be encrypted at rest.<br/>All data must be protected in transit.<br/>User's expense account number must be visible only to logged in users. All other views of the expense account number should include only the last segment, with the remaining parts obscured.<br/>In the case of a security breach, access to all summary reports must be revoked without impacting other parts of the system.<br/>**Issues -**<br/>**Upload format issue -**<br/>Employees occasionally report an issue with uploading a receipt using the web application. They report that when they upload a receipt using the Azure File Share, the receipt does not appear in their profile. When this occurs, they delete the file in the file share and use the web application, which returns a 500 Internal Server error page.<br/>**Capacity issue -**<br/>During busy periods, employees report long delays between the time they upload the receipt and when it appears in the web application.<br/>**Log capacity issue -**<br/>Developers report that the number of log messages in the trace output for the processor is too high, resulting in lost log messages.<br/>**Application code -**<br/>**Processing.cs -**<br/>`PC01 public static class Processing`<br/>`PC02 {`<br/>`PC03     public static class Function`<br/>`PC04     {`<br/>`PC05         [FunctionName("IssueWork")]`<br/>`PC06         public static async Task run([TimerTrigger("0 */5 * * * *")] TimerInfo timer, ILogger log)`<br/>`PC07         {`<br/>`PC08             var container = await GetCloudBlobContainer();`<br/>`PC09             foreach (var fileItem in await ListFiles())`<br/>`PC10             {`<br/>`PC11                 var file = new CloudFile(fileItem.StorageUri.PrimaryUri);`<br/>`PC12                 var ms = new MemoryStream();`<br/>`PC13                 await file.DownloadToStreamAsync(ms);`<br/>`PC14                 var blob = container.GetBlockblobReference(fileItem.Uri.ToString());`<br/>`PC15                 await blob.UploadFromStreamAsync(ms);`<br/>`PC16             }`<br/>`PC17         }`<br/>`PC18         private static CloudBlockBlob GetDRBlob(CloudBlockBlob sourceBlob)`<br/>`PC19         {`<br/>`PC20             ...`<br/>`PC21         }`<br/>`PC22         private async Task<CloudBlobContainer> GetCloudBlobContainer()`<br/>`PC23         {`<br/>`PC24             var CloudBlobClient = new CloudBlobClient(new Uri("..."), await GetCredentials());`<br/>`PC25`<br/>`PC26             await cloudBlobClient.GetRootContainerReference().CreateIfNotExistAsync();`<br/>`PC27             return cloudBlobClient.GetRootContainerReference();`<br/>`PC28         }`<br/>`PC29         private static async Task<StorageCredentials> GetCredentials()`<br/>`PC30         {`<br/>`PC31             ...`<br/>`PC32         }`<br/>`PC33         private static async Task<List<IListFileItem>> ListFiles()`<br/>`PC34         {`<br/>`            ...`<br/>`PC36         }`<br/>`PC37         private KeyVaultClient _KeyVaultClient = new KeyVaultClient("...");`<br/>`PC38     }`<br/>`PC39 }`<br/><br/>**Database.cs -**<br/>`DB01 public class Database`<br/>`DB02 {`<br/>`DB03     private string ConnectionString =`<br/>`DB04 `<br/>`DB05     public async Task<object> LoadUserDetails(string userId)`<br/>`DB06     {`<br/>`DB07 `<br/>`DB08             return await policy.ExecuteAsync(async () =>`<br/>`DB09             {`<br/>`DB10                 using (var connection = new SqlConnection(ConnectionString))`<br/>`DB11                 {`<br/>`DB12                     await connection.OpenAsync();`<br/>`DB13                     using (var command = new SqlCommand("...", connection))`<br/>`DB14                     using (var reader = command.ExecuteReader())`<br/>`DB15                     {`<br/>`DB16                         ...`<br/>`DB17                     }`<br/>`DB18                 }`<br/>`DB19             });`<br/>`DB20     }`<br/>`DB21 }`<br/>**ReceiptUploader.cs -**<br/>`RU01 public class ReceipUploader`<br/>`RU02 {`<br/>`RU03     public async Task UploadFile(string file, byte[] binary)`<br/>`RU04     {`<br/>`RU05         var httpClient = new HttpClient();`<br/>`RU06         var response = await httpClient.PutAsync("...", new ByteArrayContent(binary));`<br/>`RU07         while (ShouldRetry(response))`<br/>`RU08         {`<br/>`RU09             response = await httpClient.PutAsync("...", new ByteArrayContent(binary));`<br/>`RU10         }`<br/>`RU11     }`<br/>`RU12     private bool ShouldRetry(HttpReponseMessage response)`<br/>`RU13     {`<br/>`RU14`<br/>`RU15     }`<br/>`RU16 }`<br/>**ConfigureSSE.ps1 -**<br/>`CS01 $storageAccount = Gt-AzureRmStorageAccount -ResourceGroupName "..." -AccountName "..."`<br/>`CS02 $keyVault = Get-AzureRmKeyVault -VaultName "..."`<br/>`CS03 $key = Get-AzureKeyVaultKey -VaultName $keyVault.VaultName -Name "..."`<br/>`Set-AzureRmKeyVaultAccessPolicy ``\`<br/>`CS05    -VaultName $keyVault.VaultName ``\`<br/>`CS06     -ObjectId $storageAccount.Identity.PrincipalId ``\`<br/>`CS07`<br/>`CS08`<br/>`CS09 set-AzureRmStorageAccount ``\`<br/>`CS10     -ResourceGroupName $storageAccount.ResourceGroupName ``\`<br/>`CS11     -AccountName $storageAccount.StorageAccountName ``\`<br/>`CS12     -EnableEncryptionService File ``\`<br/>`CS13     -KeyvaultEncryption ``\`<br/>`CS14     -KeyName $key.Name ``\`<br/>`CS15     -KeyVersion $key.Version ``\`<br/>`CS16     -KeyVaultUri $keyVault.VaultUri`<br/>**Question**<br/>You need to ensure disaster recovery requirements are met.<br/>What code should you add at line PC16 ?<br/>`var copyOptions = new CopyOptions{};`<br/>`var context = new <Code Block1>;`<br/>`context.<Code Block2> = (source,Destination) => Task.FromResult(true);`<br/>`await TransferManager.CopyAsync(blob, GetDRBlob(blob), isServiceCopy: <Code Block3>, context: context, options:copyOptions);`<br/><br/>a. Code Block1:SingleTransferContext<br/>b. Code Block1:DirectoryTransferContext<br/>c. Code Block2:ShouldTransferCallBackAsync<br/>d. Code Block2:ShouldOverwriteCallbackAsync<br/>e. Code Block3:false<br/>f. Code Block3:true|<details><summary>Answer</summary>a. Code Block1:SingleTransferContext<br/>d. Code Block2:ShouldOverwriteCallbackAsync<br/>f. Code Block3:true<br/><br/>Scenario: Regional outage must not impact application availability. All DR operations must not be dependent on application running and must ensure that data in the DR region is up to date.<br/>The line PC16 is inside a foreach loop. It means that we need to transfer one file at at a time.<br/>SingleTransferContext is for transferring a single file.<br/>ShouldOverwriteCallbackAsync is the valid property of SingleTransferContext<br/>isServiceCopy is a flag indicating whether the copy is service-side asynchronous copy or not. If this flag is set to true, service-side asychronous copy will be used. Ideally to copy a blob, it is better to use service-side asynchronous copy operation.<br/>https://docs.microsoft.com/en-us/azure/storage/common/storage-use-data-movement-library</details>|

---

## Results
|n|Date|Note|Revision|
|-|----|----|--------|
|1|28-07-2023 PM|26/54 = 48%||
|2|01-08-2023 PM|45/54 = 83%|<details><summary>Revision</summary>True</details>|
|3|30-08-2023 AM|38/54 = 70%|<details><summary>Revision</summary>True</details>|
|4|16-09-2023 PM|38/54 = 70%|<details><summary>Revision</summary>True</details>|
